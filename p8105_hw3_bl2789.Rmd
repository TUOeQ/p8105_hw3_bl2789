---
title: "Homework 3"
author: "Bingkun Luo"
date: "10/10/2019"
output: github_document
---
# Problem 1
## a
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(p8105.datasets)
library(tidyverse)
library(data.table)
data("instacart")
```

* The instacart data set should contain `r nrow(instacart)` observations and `r ncol(instacart)` variables. and the type of varibles are listed below



```{r}
data("instacart")
str(instacart)

aisle = instacart%>%
        group_by(aisle)%>%
        summarize(count=n()) %>%
        arrange(desc(count))

most = which.max(pull(aisle,count))

```

* There are **`r max(pull(instacart,aisle_id))`** aisles and the **`r aisle$aisle[most]`** are the most items ordered from.


## b
```{r}
aisle_10k = aisle %>%
            filter(count>10000) %>%
            arrange(desc(count))

ggplot(aisle_10k,aes(x = reorder(aisle,count), y = count))+
  geom_bar(stat="identity",fill="lightblue3" )+ 
  coord_flip()+
  labs(
    title = " Number of items ordered more than 10k",
    x = "Aisle name",
    y = "Items ordered most",
    caption = "Data from INSTACART")
```

* For the products ordered more than 10,000 times,fresh vegetables and fruits are the most popular in the instacart.

## c
```{r}

pop_items = instacart %>% 
            filter(aisle == "baking ingredients"|aisle == "dog food care"|aisle ==                  "packaged vegetables fruits")%>%
            group_by(aisle,product_name)%>%
            summarize(count =n()) %>%
            top_n(3)%>%
            arrange(desc(count))
pop_items
```

* The top 3 best sells among packaged vegetables fruits are all organic foods, including Spinach, Raspberries, blueberries. 
* The top 3 baking ingredients is Light Brown Bugar,Pure Baking Soda and Cane Sugar.
* The top 3 dog fod is Snack Sticks Chicken & Rice Recipe Dog Treats,Organix Chicken & Brown Rice Recipe and Small Dog Biscuits.

## d 

```{r}
library(data.table)

Ordered_time_apple = instacart%>%
              filter(department == "produce")%>%
              filter(product_name %like% "Pink Lady")%>%
              group_by(order_dow) %>% 
              summarise(mean_hours = mean(order_hour_of_day)) %>% 
              mutate(product_name='Pink Lady Apple')%>%
              select(product_name,order_dow,mean_hours) 

Ordered_time_coffee = instacart%>%
              filter(aisle == "ice cream ice")%>%
              filter(product_name %like% "Coffee")%>%
              group_by(order_dow) %>% 
              summarise(mean_hours = mean(order_hour_of_day)) %>% 
              mutate(product_name='Coffee Ice Cream')%>%
              select(product_name,order_dow,mean_hours)

Order = rbind(Ordered_time_apple,Ordered_time_coffee)
pivot_wider(Order,names_from = order_dow,values_from = mean_hours)
  
```

*  People are most likely to buy Pink lady apple arould 13:00 and Coffee Ice cream at 14:00 in each day of the week, and also the 0 stands for Sunday, 1 for Monday,2 for Tuesday and etc. still 6 to Saturday.

# Problem 2
## Data cleaning 

```{r}
brfss_smart = brfss_smart2010%>%
                  janitor::clean_names()%>%
                  select(location_abbr = locationabbr,location_desc = locationdesc,   resp_id = respid,everything())%>%
                  filter(topic == "Overall Health")

my_levels = c("Poor","Fair","Good","Very good","Excellent")
response_adj = factor(pull(brfss_smart,response),levels = my_levels)
                  
brfss = brfss_smart%>%
        mutate(response_adj)%>%
        select(response_adj,everything(),-response)%>%
        arrange(response_adj)

```

## a
```{r}
observed_2002 = brfss%>%
           filter(year == 2002)%>%
           group_by(location_abbr)%>%
           summarize(count=n())%>%
           filter(count/5>=7)%>%
           mutate(year = 2002)
  
observed_2010 = brfss%>%
           filter(year == 2010)%>%
           group_by(location_abbr)%>%
           summarize(count=n())%>%
           filter(count/5>=7)%>%
           mutate(year = 2010)
```

*  In 2002,The states *`r pull(observed_2002,location_abbr)`* were observed at 7 or more locations. And in year 2010, *`r pull(observed_2010,location_abbr)`* were observed at 7 or more locations.

## b 
```{r}
subset = brfss%>%
         filter(response_adj == "Excellent")%>%
         select(year,location_abbr,data_value)%>%
         group_by(year,location_abbr)%>%
         mutate(average = mean(data_value))%>%
         drop_na()%>%
         select(-data_value)%>%
         distinct() 

ggplot(subset,aes(x=year,y=average,col=location_abbr))+ 
  geom_line(aes(group = location_abbr))+
  labs(
    title = "Average across years for each state",
    x = "year",
    y = "data average",
    caption = "Overall Health") 

```

* Since there are 51 states here the color is a little hard for me to distinguish from each other. The trend shows that overall health in Execent has been decreased across states. And at year 2005, we can observe a extreme decrease.

## c
```{r}
library(gridExtra)
NY_2006 = brfss%>%
          filter(year == 2006 & location_abbr == "NY")
plot_2006 = 
  ggplot(NY_2006,
         aes(x=response_adj, y = data_value,color = location_desc,group=location_desc))+
  geom_line()+
  labs(
    title = "Distribution of data_value for responses in 2006",
    x = "Response",
    y = "Data",
    caption = "Data from Newyork States")

NY_2010 = brfss%>%
          filter(year == 2010 & location_abbr == "NY")
plot_2010 = 
  ggplot(NY_2010,
         aes(x=response_adj, y = data_value,color=location_desc,group=location_desc))+
  geom_line()+
  labs(
    title = "Distribution of data_value for responses in 2010",
    x = "Response",
    y = "Data",
    caption = "Data from Newyork States")

grid.arrange(plot_2006, plot_2010, nrow = 2)
```

* The graph shows that people claim to be *Very good* or *good* are actully perform better than excellent in both year 2006 and 2010, and actually they are getting better in more recent year in the New York area, as the upper limit increases.



# Problem 3
## a
```{r}
accel_data = 
  read_csv(file = "./accel_data.csv")%>%
  janitor::clean_names()%>%
  mutate(weekday = ifelse(day %in% c("Monday", "Tuesday","Wednesday","Thursday","Friday"),1,0))%>%
  pivot_longer(activity_1:activity_1440,names_to = "minute",values_to = "activity_counts")%>%
  mutate(minute_number = as.integer(rep(c(1:1440),time = 35)))


```
* There are  `r nrow(accel_data)` observations and `r ncol(accel_data)` variables, including `r colnames(accel_data)`. 

weekday is a binary variable that I created as identifier and the minute_number is the integer for activities at nth minute. week range from 1 to 5 in this study. 



## b
```{r}

aggregate = accel_data%>%
          group_by(week,day_id,day) %>% 
          summarise(sum_of_day = sum(activity_counts))

my_levels = c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")
day_adj = factor(pull(accel_data,day),levels = my_levels)

aggregate_day = accel_data%>%
                mutate(day_adj)%>%
                group_by(week,day_id,day_adj)%>% 
                summarise(sum_of_day = sum(activity_counts))%>%
                arrange(day_adj)
aggregate_day
ggplot(aggregate_day,aes(x=week,y=sum_of_day,fill = day_adj))+
  geom_bar(stat = "identity",alpha = 0.6, position=position_dodge())+ 
  labs(
    title = "Distribution of total activity for each day",
    x = "days falls in weeks",
    y = "The total counts for one day",
    caption = "accel_data") 

  
        
```

* Are any trends apparent?
* Week four and five contribute the lowest activity on Saturday, which consider something we should look into. 

Otherwise, the daily activity is acutally random from records and there is no certain pattern to look when compiling minute activities to days. 

## c

```{r}
my_levels = c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")
day_adj = factor(pull(accel_data,day),levels = my_levels)

accel = accel_data%>%
        mutate(day_adj)%>%
        group_by(week,day_id,day_adj)
  
  
ggplot(accel, aes(x=minute_number,y=activity_counts,color = day_adj)) + 
  geom_point(alpha = 0.6) + 
  labs(
    title = "Trend of total activity in a day",
    x = "activity in nth minute",
    y = "The total counts for activity",
    caption = "accel_data") 
```

* For Thursday, the most freqent activity is on the 400th mintue which is around 6-7 am
* For Monday Wednesday and Friday, the peak is observed at 1250th minute, which is around 8-9 pm.
* For Saturday the most freqent activity is around 1000th and 1250th mintue, which is 4-9 pm.

* For Sunday the most freqent activity is around 700th mintue, which is lunch time 11am-12pm.  
